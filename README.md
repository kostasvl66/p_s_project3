# Παράλληλα Συστήματα Εργασία 3: MPI

## Άσκηση 3.1
**Υλοποίηση:** Εμμανουήλ-Ταξιάρχης Οζίνης (sdi2300147) 

### Περιγραφή προβλήματος
Το πρόβλημα μπορεί να χωριστεί στα εξής σημαντικά υποπροβλήματα:
- Την διατύπωση και υλοποίηση του σειριακού αλγορίθμου για πολλαπλασιασμό πολυωνύμων.
- Την **παραλληλοποίηση** του αλγορίθμου σε ικανοποιητικό βαθμό, δηλαδή με τέτοιο τρόπο που το παράλληλο τμήμα του είναι σημαντικά μεγαλύτερο από το μη παραλληλοποιημένο.

### Περιγραφή λύσης
Για την εκτέλεση του προγράμματος, με την βοήθεια του **Makefile** μπορούν να γίνουν δύο βασικές χρήσεις:
- Τοπική εκτέλεση σε ένα κόμβο, με ρητή χρήση των διαθέσιμων hardware threads:
    > make run-loc N=\<pol degree\> P=\<process count\>
- Εκτέλεση σε πολλούς κόμβους στο  linux lab με ρητή χρήση των διαθέσιμων πυρήνων:
    > make run-lab N=\<pol degree\> P=\<process count\>

Για την οργάνωση της λύσης, αρχικά ορίζονται οι βοηθητικές συναρτήσεις:
- `parse_args()`: Διαβάζει τα command line arguments με κατάλληλους ελέγχους. Στην συγκεκριμένη περίπτωση το μόνο argument είναι ο βαθμός των πολυωνύμων, αφού το πλήθος διεργασιών δίνεται μέσω του `mpiexec`.
- `elapsed()`: Επιστρέφει την χρονική διαφορά σε δευτερόλεπτα (έως ακρίβεια nsec) από το start στο end. Σημειώνεται ότι για τις χρονικές μετρήσεις χρησιμοποιείται η δομή struct timespec της *C11*.
- `generate_random_coef()`: Δίνει τυχαίες μη μηδενικές ακέραιες τιμές σε ένα πολυωνυμο, όπου ο κάθε συντελεστής έχει απόλυτη τιμή το πολύ `MAX_ABS_COEFFICIENT_VALUE`.

Έχει οριστεί δομή `Polynomial` και για την διαχείρησή της υπάρχουν οι ακόλουθες συναρτήσεις:
- `pol_init()`: Δεσμεύει δομή Polynomial χρησιμοποιώντας  ήδη δεσμευμένους συντελεστές.
- `pol_destroy()`:  Αποδεσμεύει το πολυώνυμο και τους συντελεστές.
- `pol_print()`: Τυπώνει ένα πολυώνυμο, για λόγους debugging
- `pol_equals()`: Ελέγχει αν δύο πολυώνυμα είναι ίσα.
- `pol_add()`: Προσθέτει δύο πολυώνυμα. Τα πολυώνυμα και το αποτέλεσμα πρέπει να είναι ήδη δεσμευμένα. Αυτό γίνεται για λόγους ταχύτητας στον πολλαπλασιασμό.
- `pol_multiply()`: Ο σειριακός αλγόριθμος πολλαπλασιασμού πολυωνύμων, ο οποίος εκτελείται μόνο από μία διεργασία. Δεσμεύει ο ίδιος το αποτέλεσμα. Η υλοποίησή του επεξηγείται αργότερα.
- `pol_multiply_parallel()`: Ο παραλληλοποιημένος αλγόριθμος πολλαπλασιασμού. Σε αντίθεση με την `pol_multiply()`, αυτή η συνάρτηση καλείται από όλα τα ranks. Όλα τα διαφορετικά code paths που σχετίζονται με τον αλγόριθμο βρίσκονται εσωτερικά σε αυτή την συνάρτηση. Η ίδια η συνάρτηση δεσμεύει το αποτέλεσμα, το οποίο είναι non-NULL μόνο για το rank 0. Η υλοποίηση του παράλληλου αλγορίθμου επεξηγείται αργότερα.

Για την υλοποίηση του σειριακού αλγορίθμου, έχει γίνει η εξής προσέγγιση:
Έστω ότι πολλαπλασιάζονται τα `pol1` και `pol2`. Τότε, διατηρώντας ένα άθροισμα αρχικοποιημένο σε 0, **για κάθε όρο** του `pol1` πολλαπλασιάζουμε αυτό τον όρο με **ολόκληρο** το `pol2`, πολλαπλασιάζοντας ή μηδενίζοντας τους κατάλληλους συντελεστές και προσαρμόζοντας τις δυνάμεις. Στο τέλος κάθε επανάληψης προσθέτουμε το προσωρινό αποτέλεσμα στο άθροισμα. Έτσι στο τέλος του αλγορίθμου το άθροισμα είναι το αποτέλεσμα του πολλαπλασιασμού.

Για την **παραλληλοποίηση** του σειριακού αλγορίθμου έχει γίνει η εξής προσέγγιση: Παρατηρείται ότι ο σειριακός αλγόριθμος αποτελείται από δύο εμφωλευμένους βρόχους. Η παραλληλοποίηση επιλέχθηκε να γίνει στον εξωτερικό βρόχο επειδή έτσι το μοναδικό σημείο που απαιτεί συνδυασμό τοπικών αποτελεσμάτων είναι η πρόσθεση πολυωνύμων και η εγγραφή σε ένα συνολικό άθροισμα στην μνήμη του rank 0. Αυτό επίσης μπορεί να βελτιωθεί αν οι προσθέσεις των "τοπικών" επαναλήψεων γίνουν σε τοπικά αθροίσματα για κάθε διεργασία, και ο τελικός συνδυασμός περιλαμβάνει μόνο μία πρόσθεση πολυωνύμων για κάθε διεργασία. Ο λόγος που δεν επιλέχθηκε ο εσωτερικός βρόχος για παραλληλοποίηση είναι ότι εκτός της επιμέρους παραλληλοποίησης και της πρόσθεσης, θα απαιτούνταν πιο σύνθετη λογική και πολλές περισσότερες επικοινωνίες. Αυτό θα γινόταν γιατί το τμήμα κώδικα που πολλαπλασιάζει έναν όρο με ένα πολυώνυμο έχει διαφορετικό αριθμό επαναλήψεων από αυτές της πρόσθεσης, πράγμα που σημαίνει ότι (για την πλήρη αξιοποίηση των διεργασιών) ο διαμοιρασμός τους θα γινόταν με διαφορετικές διεργασίες να χρησιμοποιούν αποτελέσματα άλλων διεργασιών, το οποίο θα απαιτούσε συνεχή επικοινωνία μεταξύ των διεργασιών, με μεγάλες επιβαρύνσεις. 

Στην συνάρτηση `pol_multiply_parallel()`, ο παράλληλος αλγόριθμος μπορεί να περιγραφεί με τα εξής βασικά βήματα:
- Ο rank 0 κάνει broadcast τα degrees των δύο πολυωνύμων που θα πολλαπλασιαστούν (διαφορετικά degrees για γενικότητα).
- Εφόσον παραλληλοποιείται ο εξωτερικός βρόχος (που διατρέχει το `pol1`), ο rank 0 πρέπει να στείλει μόνο τμήματα του `pol1` σε κάθε διεργασία (μέσω `MPI_Scatterv()` για να μην χρειάζεται να διαιρούνται τέλεια τα δεδομένα). Από την άλλη το `pol2` πρέπει να γίνει broadcast γιατί διαβάζεται ολόκληρο από όλες τις διεργασίες. Όσον αφορά το `pol1`, στόχος του αλγορίθμου είναι οι συντελεστές του να μοιράζονται κυκλικά σε κάθε διεργασία για καλύτερο load balancing. Επομένως υπάρχει βήμα προεπεξεργασίας των συντελεστών του `pol1`, και επαναδιάταξή τους σε ένα `packed_chunks` buffer που ομαδοποιεί σε contiguous chunks τα ισοϋπόλοιπα indexes (mod rank) για κάθε rank.
- Γίνονται δεσμεύσεις μνήμης (μόνο για τοπική χρήση στον αλγόριθμο), οι οποίες επεξηγούνται αργότερα.
- Εκτελούνται οι εμφωλευμένοι βρόχοι τοπικά σε κάθε διεργασία.
- Κάθε μη μηδενικό rank στέλνει το τοπικό του άθροισμα στο rank 0, το οποίο αναλαμβάνει τις τελικές προσθέσεις και υπολογίζει το συνολικό αποτέλεσμα. 

Σημειώνεται ότι στην συνάρτηση `pol_multiply_parallel()` γίνονται πριν την εκτέλεση των βρόχων αρκετές δεσμεύσεις μνήμης, έτσι ώστε να αποφευχθούν όσο το δυνατότερο δεσμεύσεις μέσα σε βρόχους του αλγορίθμου. Ωστόσο αυτές οι δεσμεύσεις θεωρούνται τμήμα του αλγορίθμου όσον αφορά τις μετρήσεις, επειδή ο σειριακός αλγόριθμος δεν έχει κατι αντίστοιχο, και επομένως αυτό πρέπει να θεωρηθεί overhead του παράλληλου αλγορίθμου. Επισης σημειώνεται ότι ο κώδικας για την δέσμευση αυτών των μεταβλητών έχει αλλάξει σε ένα βαθμό σε σχέση με το αντίστοιχο κώδικα της άσκησης 2.1 της εργασίας 2 (χρήση του `SAFE_CALL()` macro). Αυτή η αλλαγή έχει γίνει για λόγους καλύτερης οργάνωσης του κώδικα και δεν έχει επιρροή στον χρόνο εκτέλεσης, που σημαίνει ότι η σύγκριση της άσκησης 3.1 με την 1.1 ή 2.1 μπορεί να γίνει με συνεπή τρόπο για τον χρόνο εκτέλεσης.

### Πειραματικά αποτελέσματα
- **N**: Βαθμός των πολυωνύμων
- **P**: Αριθμός διεργασιών στον παράλληλο αλγόριθμο
- Σε όλα τα παρακάτω αποτελέσματα οι χρόνοι είναι σε *seconds* και δίνεται ακρίβεια *microsecond*.
- Αποφεύγεται η χρήση του N = *10^6* γιατί στο συγκεκριμένο σύστημα μια τέτοια εκτέλεση ολοκληρώνεται σε μη πρακτικό αριθμό λεπτών, καθώς η πολυπλοκότητα είναι τετραγωνική.
- Στα πειράματα με πολλούς κόμβους χρησιμοποιούνται μεγαλύτερα πλήθη από διεργασίες, συγκεκριμένα μέχρι και 64, αφού λόγω του διαχωρισμού των δεδομένων όχι μόνο ανά νήμα αλλά και ανά κόμβο, σε πολύ μεγάλο βαθμό σταματά να υπάρχει contention μεταξύ των διεργασιών. Αυτό σημαίνει ότι το efficiency μειώνεται μόνο σε μεγάλους αριθμούς από processes, και το 64 έχει επιλεχθεί ως ένα όριο με ένα σχετικά χαμηλό efficiency. Σημειώνεται ότι συγκεκριμένα για τα linux του εργαστηρίου (linux01 - linux30) όπου το καθένα έχει 4 πυρήνες-threads, το άνω όριο σε πραγματική παραλληλία θα ήταν 120 processes.
- Στην σύγκριση για μόνο έναν κόμβο, ο μέγιστος αριθμός από processes που χρησιμοποιείται στα πειράματα είναι **15** και όχι 16. Αυτό συμβαίνει λόγω της naive προσέγγισης που έχει γίνει στην άσκηση 1.1 με pthreads, επειδή το main thread που δημιουργεί τα pthreads δεν εκτελεί καμία "χρήσιμη" εργασία, αλλά απλώς περιμένει τα υπόλοιπα threads. Επίσης αυτό σημαίνει ότι τα πειράματα με 16 threads στην άσκηση 1.1 τεχνικά χρησιμοποιούσαν 17 threads, δηλαδή ένα παραπάνω νήμα που δεν εκτελούνταν πραγματικά παράλληλα με όλα τα υπόλοιπα. Αντίθετα, η άσκηση 3.1 με MPI αξιοποιεί όλα τα threads, συμπεριλαμβανομένου του main thread, και μπορεί πράγματι να τρέξει τον αλγόριθμο με 16 hardware threads (όπου αντιστοιχίζονται τα processes). Για να γίνει η σύγκριση σωστά όμως, εδώ θα χρησιμοποιηθούν έως 15 processes/threads και για τις δύο υλοποιήσεις.

Για τον παράλληλο αλγόριθμο ο συνολικός χρόνος μπορεί να χωριστεί σε τρεις φάσεις:
1. **Splitting/sending**: Γενικά η φάση αποστολής όλων των απαραίτητων δεδομένων στις διεργασίες από τον rank 0. Αυτό περιλαμβάνει και την κατάλληλη προεπεξεργασία όσων δεδομένων πρέπει να αποσταλούν με συγκεκριμένο τρόπο, όπως είναι η δημιουργία του `packed_chunks`.
2. **Computations**: Η φάση των αυτόνομων υπολογισμών για κάθε διεργασία παράλληλα, που αποτελεί το κυρίαρχο τμήμα του χρόνου εκτέλεσης. Επίσης περιλαμβάνει μερικές δεσμεύσεις μνήμης που χρησιμεύουν στον αλγόριθμο και διατηρούνται μονο τοπικά για κάθε διεργασία.
3. **Receiving/combining**: Η φάση κατά την οποία τα ήδη έτοιμα τοπικά αποτελέσματα αποστέλλονται στο rank 0, και αυτό τα συνδυάζει για να υπολογίσει το συνολικό αποτέλεσμα. Αυτό το τμήμα δεν περιλαμβάνει τις τελικές αποδεσμεύσεις μνήμης. Αυτές συνυπολογίζονται μόνο από τον συνολικό παράλληλο χρόνο.

Οι παρακάτω πίνακες αναφέρονται σε πειράματα που έγιναν χρησιμοποιώντας πολλούς κόμβους του εργαστηρίου linux.

Χρόνοι για **splitting/sending** για κάθε πείραμα με συγκεκριμένο N και P.
|**N\P**      |**4**   |**8**   |**16**  |**32**  |**64**  |
|-------------|--------|--------|--------|--------|--------|
|**10000**    |0.000523|0.002427|0.007347|0.012789|0.016014|
|**100000**   |0.002330|0.011846|0.022871|0.032288|0.035687|

Χρόνοι για **computations** για κάθε πείραμα με συγκεκριμένο N και P.
|**N\P**      |**4**   |**8**   |**16**  |**32**  |**64**  |
|-------------|--------|--------|--------|--------|--------|
|**10000**    |0.056805|0.034396|0.024946|0.013559|0.003490|
|**100000**   |5.772085|2.829935|1.420478|0.852273|0.402394|

Χρόνοι για **receiving/combining** για κάθε πείραμα με συγκεκριμένο N και P.
|**N\P**      |**4**   |**8**   |**16**  |**32**  |**64**  |
|-------------|--------|--------|--------|--------|--------|
|**10000**    |0.000698|0.006357|0.002050|0.242087|0.224595|
|**100000**   |0.020286|2.256214|1.098981|0.637374|0.826898|

Σειριακοί χρόνοι εκτέλεσης για κάθε πείραμα με συγκεκριμένο N και P.
|**N\P**      |**4**    |**8**    |**16**   |**32**   |**64**   |
|-------------|---------|---------|---------|---------|---------|
|**10000**    |0.189322 |0.189159 |0.189530 |0.187855 |0.187474 |
|**100000**   |19.184240|19.082744|19.135596|19.755201|19.665321|

Παράλληλοι χρόνοι εκτέλεσης για κάθε πείραμα με συγκεκριμένο N και P, σε αντιστοιχία με τον παραπάνω πίνακα.
|**N\P**      |**4**   |**8**   |**16**  |**32**  |**64**  |
|-------------|--------|--------|--------|--------|--------|
|**10000**    |0.058205|0.046031|0.035546|0.270807|0.256188|
|**100000**   |5.794912|5.102816|2.543411|1.532030|1.283519|

*Speedup* παράλληλης εκτέλεσης σε σχέση με την σειριακή, σε αντιστοιχία με τους παραπάνω πίνακες.
|**N\P**      |**4**   |**8**   |**16**  |**32**   |**64**   |
|-------------|--------|--------|--------|---------|---------|
|**10000**    |3.252695|4.10938 |5.331889|0.693685 |0.731786 |
|**100000**   |3.310532|3.739650|7.523596|12.894783|15.321404|

*Efficiency* σε σχέση με το *speedup* και τον αριθμό των νημάτων, σε αντιστοιχία με τους παραπάνω πινακες.
|**N\P**      |**4**   |**8**   |**16**  |**32**  |**64**  |
|-------------|--------|--------|--------|--------|--------|
|**10000**    |0.827633|0.467456|0.470225|0.021678|0.011434|
|**100000**   |0.827633|0.467456|0.470225|0.402962|0.239397|

### Σχολιασμός αποτελεσμάτων
Για τα παραπάνω αποτελέσματα να σημειωθεί ότι λόγω της συνύπαρξης πολλών χρηστών στα συστήματα linux, δεν ήταν εύκολο να βρεθεί ιδανική στιγμή για μετρήσεις. Επομένως οι παραπάνω τιμές δεν αντιστοιχούν στις πραγματικές δυνατότητες των μηχανημάτων, καθώς σε προηγούμενες δοκιμές έχουν παρατηρηθεί και αισθητά καλύτερες επιδόσεις.

Από τους παραπάνω πίνακες συμπεραίνουμε τα εξής:
- Γενικά  υπάρχει αισθητή επιτάχυνση, η οποία είναι πολύ σημαντικότερη για μεγαλύτερες τιμές του *Ν*.
- Ήδη από τις 8 διεργασίες και άνω παρατηρείται σημαντική πτώση στην αποδοτικότητα. Παρόλα αυτά για ένα μεγάλο διάστημα, μέχρι περίπου τις 32 διεργασίες η αποδοτικότητα μειώνεται ελάχιστα, και η επόμενη σημαντική μείωση παρατηρείται μετά από αυτά. Το συγκεκριμένο φαινόμενο πιθανότατα οφείλεται στην συνύπαρξη πολλών χρηστών, διότι σε μεμονωμένες δοκιμές (όχι συνολικά) πριν από την τελική μέτρηση έχει παρατηρηθεί και πιο αναμενόμενη μείωση του χρόνου από 4 σε 8 διεργασίες.
- Στην φάση splitting/sending, όπως είναι αναμενόμενο, οι χρόνοι είναι σχετικά μικροί όμως αυξάνονται είτε με την αύξηση των διεργασιών είτε με την αύξηση του N.
- Στην φάση computations οι χρόνοι μειώνονται πιο προβλέψιμα από τους συνολικούς, και συγκεκριμένα για N=10^5, οι χρόνοι φαίνονται περίπου να υποδιπλασιάζονται, όσο οι διεργασίες διπλασιάζονται.
- Στην φάση receiving/combining οι χρόνοι για N=10^5 φαίνονται αρκετά απρόβλεπτοι, και ίσως αυτό οφείλεται σε internal υλοποιήσεις των μηχανισμών επικοινωνίας μεταξύ των διεργασιών. Μια αιτία θα μπορούσε να είναι, για παράδειγμα, το αν μια επικοινωνία μεταξύ δύο διεργασιών πρέπει να γίνει μέσω τοπικού δικτύου (σε περίπτωση διαφορετικών κόμβων) ή μέσω μηχανισμών IPC του λειτουργικού συστήματος (σε περίπτωση που είναι στον ίδιο κόμβο).
- Στην φάση receiving/combining, για N=10^5 υπάρχει μια ξαφνική αύξηση του χρόνου απο 4 σε 8 διεργασίες. Αυτή η ξαφνική αύξηση είναι ο άμεσος λόγος για τον οποίο το συνολικό speedup από 4 σε 8 διεργασίες δεν αλλάζει σχεδόν καθόλου. Παρόλα αυτά, ο έμμεσος λόγος, όπως αναφέρθηκε και πριν, είναι πιθανότατα ο ανταγωνισμός μεταξύ διεργασιών διαφορετικών χρηστών για πόρους.

#### Σύγκριση με υλοποίηση `pthreads`
Εφόσον η υλοποίηση με pthreads ήταν αποδοτικότερη από την υλοποίηση με OpenMP, η σύγκριση της υλοποίησης με MPI θα συγκριθεί με αυτή των pthreads.

Οι παρακάτω πίνακες αναφέρονται σε πειράματα που έγιναν τοπικά σε έναν μόνο κόμβο, τα χαρακτηριστικά του οποίου παρατίθονται στο τέλος της αναφοράς 3.1.

Υλοποίηση **pthreads**: *Speedup* παράλληλης εκτέλεσης σε σχέση με την σειριακή
|**N\Threads**|**1**   |**2**   |**4**   |**8**   |**15**  |
|-------------|--------|--------|--------|--------|--------|
|**1000**     |0.791199|1.239546|1.288399|1.229979|0.969816|
|**10000**    |1.103307|2.099371|3.287014|3.734003|4.312176|
|**100000**   |1.118343|2.216025|3.914853|6.188193|7.613948|

Υλοποίηση **pthreads**: *Efficiency* σε αντιστοιχία με τον προηγούμενο πίνακα.
|**N\Threads**|**1**   |**2**   |**4**   |**8**   |**15**  |
|-------------|--------|--------|--------|--------|--------|
|**1000**     |0.791199|0.619773|0.322099|0.153747|0.064654|
|**10000**    |1.103307|1.049685|0.821753|0.466750|0.287478|
|**100000**   |1.118343|1.108012|0.978713|0.773524|0.507596|

Υλοποίηση **MPI**: *Speedup* παράλληλης εκτέλεσης σε σχέση με την σειριακή
|**N\P**      |**1**   |**2**   |**4**   |**8**   |**15**  |
|-------------|--------|--------|--------|--------|--------|
|**1000**     |0.962349|1.796098|2.728352|0.724516|0.546225|
|**10000**    |1.039938|1.923327|3.557693|3.725310|5.765541|
|**100000**   |1.061416|2.096624|3.778442|4.972481|6.018304|

Υλοποίηση **MPI**: *Efficiency* σε αντιστοιχία με τον προηγούμενο πίνακα.
|**N\P**      |**1**   |**2**   |**4**   |**8**   |**15**  |
|-------------|--------|--------|--------|--------|--------|
|**1000**     |0.962349|0.898049|0.682088|0.090565|0.036415|
|**10000**    |1.039938|0.961663|0.889423|0.465664|0.384369|
|**100000**   |1.061416|1.048312|0.944610|0.621560|0.401220|

Παρατηρούμε ότι μεταξύ των δύο υλοποιήσεων, η υλοποίηση με **pthreads** είναι αποδοτικότερη από αυτή του MPI, αφού για μεγαλύτερα N οι επιταχύνσεις με pthreads είναι σχεδόν όλες μεγαλύτερες από τις αντίστοιχες με MPI. Αυτό είναι αναμενόμενο γιατί το MPI είναι σχεδιασμένο για παραλληλία σε μεγαλύτερη κλίμακα (και όχι απλά μεταξύ νημάτων σε ένα κόμβο) και απαιτεί αναγκαία overheads, το σημαντικότερο από τα οποία είναι η επικοινωνία μεταξύ διεργασιών. Αντίθετα, τα pthreads είναι πιο low level και εστιάζουν στην χρήση νημάτων για ένα μόνο σύστημα κοινόχρηστης μνήμης, με αποτέλεσμα η χρήση των νημάτων να είναι προσδιορισμένη με περισσότερη ακρίβεια, που συνεπάγεται καλύτερη αποδοτικότητα.

Για τα πειράματα σε ένα κόμβο (MPI/pthreads) χρησιμοποιήθηκε σύστημα με τα εξής χαρακτηριστικά:
- **Μοντέλο laptop:** MSI Katana GF66 12UC
- **Μοντέλο CPU:** 12th Gen Intel(R) Core(TM) i7-12650H
- **Logical processors:** 16
- **OS:** WSL2 (Ubuntu) πάνω σε Windows11
- **C compiler:** gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0

**Σημείωση:** Η hardware μνήμη RAM είναι 16GB όμως το WSL την περιορίζει σε ~8GB. Παρόλα αυτά **τονίζεται ότι το WSL δεν περιορίζει την χρήση των hardware threads**. 

## Άσκηση 3.2

### Χαρακτηριστικά υλοποίησης
- Υλοποίηση προγράμματος από: Κωνσταντίνο Γεώργιο Βλαζάκη (sdi2300017)
- Λειτουργικό σύστημα όπου εκτελέστηκαν τα προγράμματα: Linux Mint 22.2 Cinnamon
- CPU: AMD Ryzen 5 7600 6-Core Processor
- GCC version: gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
